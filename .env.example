# Server Configuration
PORT=4242

# Backend Configuration
# Option 1: Cloud (zen) - uses free GLM 4.7 model
BACKEND_URL=https://opencode.ai/zen/v1

# Option 2: Local (llama.cpp) - requires running llama-server
# BACKEND_URL=http://localhost:8080/v1

# Logging Level
# Options: DEBUG, INFO, WARN, ERROR
LOG_LEVEL=INFO
